# -*- coding: utf-8 -*-
"""Uplift Modelling .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qk6LDnvuzACkxwuAVH4ZHO1V8waOmzpa
"""

!pip install pandas-profiling
!pip install causalml
!pip install scikit-plot

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, roc_auc_score, classification_report
from ydata_profiling import ProfileReport

import scikitplot as skplt

from sklearn.tree import DecisionTreeClassifier
from lightgbm import LGBMClassifier

from causalml.inference.meta import BaseSClassifier
from causalml.dataset import make_uplift_classification
from causalml.inference.tree import UpliftRandomForestClassifier
import causalml.metrics as metrics
from causalml.inference.tree import uplift_tree_string, uplift_tree_plot

import pandas as pd

df = pd.read_csv('/content/data.csv')

df.head()  # Display first few rows

df.info()  # Data types and non-null counts

df.describe()  # Summary statistics

# Data Profiling
profile = ProfileReport(df, title="Profiling Report")
profile.to_notebook_iframe()

categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    plt.figure(figsize=(8, 6))
    sns.countplot(x=col, hue='offer', data=df)
    plt.title(f'Distribution {col} vs Target Variable')
    plt.show()

def num_summary(dataframe, numerical_col, plot=False):
    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]
    print(dataframe[numerical_col].describe(quantiles).T)

    if plot:
        colors = [ "#F5801A","#E3CFC2", "#202240", "#FFFFFF", "#838496"]
        sns.set_palette(sns.color_palette(colors))
        sns.histplot(x=dataframe[numerical_col], data=dataframe)
        plt.show()

#showing conversion rate customer by offer

#Counting offer level (offer)
conversion_by_offer = df.groupby('offer')['conversion'].mean().reset_index()

# Membuat diagram batang horizontal
plt.figure(figsize=(10, 5))
sns.barplot(x='conversion', y='offer', data=conversion_by_offer, palette='viridis')
plt.title('Conversion Rate by Offer')
plt.xlabel('Convertion Rate')
plt.ylabel('offer')
plt.show()

# Assuming 'used_discount' is a binary column where 1 indicates usage and 0 indicates non-usage
plt.figure(figsize=(9, 5))
sns.countplot(data=df, x='used_discount', palette=['#1f77b4', '#ff7f0e'])  # Blue for not used, orange for used
plt.title('Distribution of Discount Usage')
plt.xlabel('Used Discount')
plt.ylabel('Count')
plt.xticks([0, 1], ['No', 'Yes'])  # Assuming 0 for no and 1 for yes

# Show the plot
plt.show()

#Create the bar chart Referral Distribution
plt.figure(figsize=(9, 5))
sns.countplot(data=df, x='is_referral', palette='viridis')  # Choose a color palette
plt.title('Referral Distribution')
plt.xlabel('Referral')
plt.ylabel('Frequency')
plt.xticks(rotation=45)  # Rotate x-axis labels if there are many categories

# Show the plot
plt.show()

# Create the bar chart onversion
plt.figure(figsize=(9, 5))
sns.countplot(data=df, x='conversion', palette='viridis')  # Choose a color palette
plt.title('Conversion Distribution')
plt.xlabel('Conversion')
plt.ylabel('Frequency')
plt.xticks(rotation=45)  # Rotate x-axis labels if there are many categories

# Show the plot
plt.show()

# Create the bar chart onversion
plt.figure(figsize=(8, 4))
sns.countplot(data=df, x='channel', palette='viridis')  # Choose a color palette
plt.title('Channel Distribution')
plt.xlabel('Channel')
plt.ylabel('Frequency')
plt.xticks(rotation=45)  # Rotate x-axis labels if there are many categories

# Show the plot
plt.show()

# Create the bar chart onversion
plt.figure(figsize=(8, 4))
sns.countplot(data=df, x='zip_code', palette='viridis')  # Choose a color palette
plt.title('Zip Code Distribution')
plt.xlabel('Zip Code')
plt.ylabel('Frequency')
plt.xticks(rotation=45)  # Rotate x-axis labels if there are many categories

# Show the plot
plt.show()

"""#EDA"""

# treatment variable
treatment_variable = "offer"
# target variable
target_variable = "conversion"
# control category
control_category = "No Offer"
# treatment category
treatment_category = "Discount"
# category to exclude if needed
exclusion_category = "Buy One Get One"
# column to drop
variable_to_drop = ['is_referral','used_discount','used_bogo']
# column to encode
categorical_columns = ["zip_code", "channel"]

df.head()

sns.countplot(data = df, x = treatment_variable);

df = df.loc[df[treatment_variable] != 'Buy One Get One'].reset_index(drop=True)

df.head()

df.offer.value_counts()

"""#Split Data"""

X_train, X_test = train_test_split(
    df.drop(variable_to_drop, axis = 1),
    test_size = 0.5,
    random_state = 42
)

"""#Data Preprocessing and Data Exploration"""

X_train.head()

X_test.head()

X_train.offer.value_counts(normalize = True).plot.barh();

# Encode categorical variables on X train data
dummies = pd.get_dummies(X_train[categorical_columns], drop_first=True)
X_train = pd.concat([X_train.drop(categorical_columns, axis=1), dummies], axis=1)

# Encode categorical variables on X test data
dummies = pd.get_dummies(X_test[categorical_columns], drop_first=True)
X_test = pd.concat([X_test.drop(categorical_columns, axis=1), dummies], axis=1)

X_train

control_category

treatment_category

is_treat = X_train[X_train.offer != control_category]
not_treat = X_train[X_train.offer == control_category]

not_treat.head()

is_treat.head()

# bins = 25
sns.distplot(is_treat.query("offer == 'Discount'").history, hist=True, kde=True, kde_kws={'linewidth': 4}, label='Discount')
sns.distplot(is_treat.query("offer == 'No Offer'").history, hist=True, kde=True, kde_kws={'linewidth': 4}, label='No Offer')
# sns.distplot(not_treat.history, hist=True, kde=True, kde_kws={'linewidth': 4}, label='control')
plt.legend(frameon=False, loc=0, ncol=1, prop={'size': 20});

# bins = 25
sns.distplot(is_treat.recency, hist=True, kde=True, kde_kws={'linewidth': 4}, label='treatment')
sns.distplot(not_treat.recency, hist=True, kde=True, kde_kws={'linewidth': 4}, label='control')
plt.legend(frameon=False, loc=0, ncol=1, prop={'size': 20});

"""#Modeling"""

X_train.head()

target_variable

treatment_variable

x_col = X_train.drop([treatment_variable, target_variable], axis = 1).columns.tolist()
print(x_col)

"""#S-Learner

Define S-Learner (Classifier) using LGBMClassifier as base model, or using XGBoostClassifier or CatBoostClassifier
"""

control_category

slearner = BaseSClassifier(LGBMClassifier(), control_name=control_category)

slearner

"""**Estimate Average Treatment Effect**"""

slearner.estimate_ate(X_train[x_col].values, X_train[treatment_variable].values, X_train[target_variable].values, bootstrap_ci  = True)

slearner_tau = slearner.predict(X_test[x_col].values, X_test[treatment_variable].values, X_test[target_variable].values)

slearner_tau

X_test['s_learner_tau'] = slearner_tau

X_test

sns.displot(data = X_test['s_learner_tau'])
plt.vlines([0], 0, 1400, linestyles = "dashed", colors = "red")
plt.xlabel('Uplift score')
plt.ylabel('Number of observations in validation set');

"""#Uplift Tree Based"""

X_train.head()

target_variable

treatment_variable

x_col = X_train.drop([treatment_variable, target_variable], axis = 1).columns.tolist()
print(x_col)

"""Define uplift random forest classifier

"""

uplift_model = UpliftRandomForestClassifier(control_name=control_category, random_state=1000)

uplift_model.fit(
    X_train[x_col].values,
    treatment = X_train[treatment_variable].values,
    y = X_train[target_variable].values
)

"""Do prediction with trained model"""

y_pred = uplift_model.predict(X_test[x_col].values, full_output=True)

"""Check the first 5 rows"""

y_pred.head()

X_test['uplift_forest_tau'] = uplift_model.predict(X_test[x_col].values, full_output=False)

X_test.head()

"""Check uplift distribution"""

sns.displot(data = X_test['uplift_forest_tau'])
plt.vlines([0], 0, 1400, linestyles = "dashed", colors = "red")
plt.title('Uplift Tree Distribution')
plt.xlabel('Uplift score')
plt.ylabel('Number of observations in validation set');

sns.displot(data = X_test['s_learner_tau'])
plt.vlines([0], 0, 1400, linestyles = "dashed", colors = "red")
plt.title('Uplift S-Learner Distribution')
plt.xlabel('Uplift score')
plt.ylabel('Number of observations in validation set');

"""#Model Evalation"""

def auuc_metric_maker(dataframe, tau_outcome_var, control_category, treatment_category):

  treatment_category_result = X_test[[tau_outcome_var]].reset_index(drop=True)
  treatment_category_result.columns = [treatment_category]

  # If all deltas are negative, assing to control; otherwise assign to the treatment
  # with the highest delta
  best_treatment = np.where(
      (treatment_category_result < 0).all(axis=1),
      control_category,
      treatment_category_result.idxmax(axis=1)
  )
  # Create indicator variables for whether a unit happened to have the
  # recommended treatment or was in the control group
  actual_is_best = np.where(dataframe[treatment_variable] == best_treatment, 1, 0)
  actual_is_control = np.where(dataframe[treatment_variable] == control_category, 1, 0)

  synthetic = (actual_is_best == 1) | (actual_is_control == 1)
  synth = treatment_category_result[synthetic]

  auuc_score = (synth.assign(
      is_treated = 1 - actual_is_control[synthetic],
      conversion = dataframe.loc[synthetic, target_variable].values,
      model_result = synth.max(axis=1)
  ).drop(columns=list([treatment_category]))).rename(columns = {"model_result": tau_outcome_var})

  return auuc_score

"""#SLearner Evaluation"""

slearner_auuc_score = auuc_metric_maker(X_test, tau_outcome_var = "s_learner_tau", control_category = control_category, treatment_category = treatment_category)

slearner_auuc_score.head()

"""Calculate treated group based who visit our platform, treated or not"""

slearner_auuc_score.groupby('is_treated').sum()[[target_variable]]

"""#Uplift Forest Evaluation"""

uplift_forest_auuc_score = auuc_metric_maker(X_test, tau_outcome_var = "uplift_forest_tau", control_category = control_category, treatment_category = treatment_category)

uplift_forest_auuc_score.head()

uplift_forest_auuc_score.groupby('is_treated').sum()[[target_variable]]

"""#Comulative Gain Plot

**SLearner Model**
"""

metrics.plot_gain(slearner_auuc_score, outcome_col=target_variable, treatment_col='is_treated')

metrics.plot_gain(uplift_forest_auuc_score, outcome_col=target_variable, treatment_col='is_treated')

"""# AUUC and Qini Score

**AUUC for S-Learner**
"""

metrics.auuc_score(slearner_auuc_score, outcome_col=target_variable, treatment_col='is_treated')

"""AUUC for Uplift Forest"""

metrics.auuc_score(uplift_forest_auuc_score, outcome_col=target_variable, treatment_col='is_treated')

"""**Qini Score for S-Learner**"""

metrics.qini_score(slearner_auuc_score, outcome_col=target_variable, treatment_col='is_treated')

"""**Qini Score for Uplift Forest**"""

metrics.qini_score(uplift_forest_auuc_score, outcome_col=target_variable, treatment_col='is_treated')

"""# Quantile Metrics

Create new dataframe object
"""

X_test

def quantile_and_treatment(dataframe, tau_outcome_var, treatment_variable, control_category, treatment_category):
  # Bin uplift score by using quantile
  score_quantiles, score_quantile_bins = pd.qcut(
    x = dataframe[tau_outcome_var],
    q = 10,
    retbins = True,
    duplicates = 'drop'
  )
  dataframe['Quantile bin'] = score_quantiles
  # Calculate number of samples for each bins
  count_by_quantile_and_treatment = dataframe.groupby(['Quantile bin', treatment_variable])[treatment_variable].count().unstack(-1)
  return count_by_quantile_and_treatment[[control_category, treatment_category]]

final_result = quantile_and_treatment(X_test, tau_outcome_var = "uplift_forest_tau", treatment_variable = treatment_variable, control_category = control_category, treatment_category = treatment_category)

"""**Visualize the impact of the treatment**"""

final_result.plot.barh()
plt.xlabel('Number of observations');

"""#Uplift Quantil Chart"""

def true_uplift(dataframe, tau_outcome_var, target_variable, treatment_variable, treatment_category, ):
  # Bin uplift score by using quantile
  score_quantiles, score_quantile_bins = pd.qcut(
    x = dataframe[tau_outcome_var],
    q = 10,
    retbins = True,
    duplicates = 'drop'
  )

  dataframe['Quantile bin'] = score_quantiles
  # Get the conversion rates within uplift score quantiles for both groups
  validation_treatment_mask = dataframe[treatment_variable] == treatment_category
  treatment_by_quantile = dataframe[validation_treatment_mask]\
    .groupby('Quantile bin')[target_variable].mean()
  control_by_quantile = dataframe[~validation_treatment_mask]\
    .groupby('Quantile bin')[target_variable].mean()
  # calculate true uplift
  true_uplift_by_quantile = treatment_by_quantile - control_by_quantile
  return true_uplift_by_quantile

true_uplift_result = true_uplift(X_test, tau_outcome_var = "uplift_forest_tau", target_variable = target_variable, treatment_variable = treatment_variable, treatment_category = treatment_category)

true_uplift_result.head(5)

"""Visualize uplift quantile chart"""

plt.rcParams["figure.figsize"] = (8, 6)
true_uplift_result.plot.barh()
plt.xlabel('True uplift');

"""#Model Intrepretation

## Feature Importance (Gain, Permutation, etc)
"""

slearner.plot_importance(X=X_test[x_col],
                        tau=X_test['s_learner_tau'],
                        method='auto',
                        random_state = 42,
                        features=x_col)

"""## Shapley Dependence Model

**Shap Value for History**
"""

print(X_test.columns)

slearner.plot_shap_dependence(
    treatment_group=treatment_category,
    feature_idx='history',
    X=X_test[x_col].values,
    features = x_col,
    tau= X_test['s_learner_tau'],
    interaction_idx=None
)

"""## Shap Value for used discount"""

slearner.plot_shap_dependence(
    treatment_group=treatment_category,
    feature_idx='recency',
    X=X_test[x_col].values,
    features = x_col,
    tau= X_test['s_learner_tau'],
    interaction_idx=None
)

slearner.plot_shap_dependence(
    treatment_group=treatment_category,
    feature_idx='zip_code_Surburban',
    X=X_test[x_col].values,
    features = x_col,
    tau= X_test['s_learner_tau'],
    interaction_idx=None
)

"""# Special Explanation Method for Tree-Based Model

## Feature Importance
"""

pd.DataFrame(
    {
        "Variable": x_col,
        "Importance": uplift_model.feature_importances_
    }
).sort_values(by="Importance", ascending = True).plot(x = 'Variable', y = 'Importance', kind = 'barh');

